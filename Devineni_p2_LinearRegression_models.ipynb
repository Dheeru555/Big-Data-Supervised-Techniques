{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for 2015 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkSession is the entry point to Spark SQL. It is the very first object \n",
    "#to create while developing Spark SQL applications.\n",
    "#Used the SparkSession.builder method to create an instance of SparkSession with appName('WorldHealth')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('WorldHealth').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the HR_comma_sep.csv file into df1 dataframe.\n",
    "df1 = spark.read.csv ('WH_2015.csv', inferSchema=True, header =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It displays the schema of the dataframe df1\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the module StringIndexer from subpackage ml.feature\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In first step by using StringIndexer function we are creating an output label 'Country_Lab' with input 'Country', as we need integer values for performing logistic regression\n",
    "#We are transforming the above result into indexed dataframe and showing the first 10 results.\n",
    "indexer = StringIndexer(inputCol='Country', outputCol='Country_Lab')\n",
    "indexed = indexer.fit(df1).transform(df1)\n",
    "indexed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the modules Vectors & VectorAssembler from subpackage ml.linalg & ml.feature\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the columns in the indexed dataframe\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By using VectorAsssembler creating features output column with input columns including everything except label column\n",
    "#Happiness Rank and Region which is not much required for prediction\n",
    "assembler = VectorAssembler(inputCols=[\n",
    " 'Happiness Score',\n",
    " 'Standard Error',\n",
    " 'Economy (GDP per Capita)',\n",
    " 'Family',\n",
    " 'Health (Life Expectancy)',\n",
    " 'Freedom',\n",
    " 'Trust (Government Corruption)',\n",
    " 'Generosity',\n",
    " 'Dystopia Residual',\n",
    " 'Country_Lab'\n",
    "], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the indexed dataframe to the dataframe output\n",
    "output = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only required columns or features into final data dataframe\n",
    "final_data = output.select(['features', 'Happiness Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the actual data into traindata & test data which is of 70% & 30%\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the LinearRegression module from the regression subpackage\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the label coulmn for this problem as 'Happiness Rank' and applying on LinearRegression function \n",
    "#and assigining to HR dataframe.\n",
    "HR = LinearRegression(labelCol='Happiness Rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fit the model using hyper-parameter - regparam of 100\n",
    "trained_HR_model = HR.fit(final_data,{lr.regParam:100.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fit the model on test data and storing it to HR_results dataframe for measuring the accuracy of that model\n",
    "HR_results = trained_HR_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the metric RMSE value\n",
    "HR_results.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the accuracy\n",
    "HR_results.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform the final_data into predictionsA and show the predictions\n",
    "predictionsA = trained_HR_model.transform(final_data)\n",
    "display(predictionsA)\n",
    "predictionsA.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for 2016 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkSession is the entry point to Spark SQL. It is the very first object \n",
    "#to create while developing Spark SQL applications.\n",
    "#Used the SparkSession.builder method to create an instance of SparkSession with appName('WorldHealth')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('WorldHealth1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the HR_comma_sep.csv file into df1 dataframe.\n",
    "df2 = spark.read.csv ('WH_2016.csv', inferSchema=True, header =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It displays the schema of the dataframe df1\n",
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the module StringIndexer from subpackage ml.feature\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In first step by using StringIndexer function we are creating an output label 'Country_Lab' with input 'Country', as we need integer values for performing logistic regression\n",
    "#We are transforming the above result into indexed dataframe and showing the first 10 results.\n",
    "indexer = StringIndexer(inputCol='Country', outputCol='Country_Lab')\n",
    "indexed = indexer.fit(df2).transform(df2)\n",
    "indexed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the modules Vectors & VectorAssembler from subpackage ml.linalg & ml.feature\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the columns in the indexed dataframe\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By using VectorAsssembler creating features output column with input columns including everything except label column\n",
    "#Happiness Rank and taken columns that re common with 2015 data.\n",
    "assembler = VectorAssembler(inputCols=[\n",
    " 'Happiness Score',\n",
    " 'Economy (GDP per Capita)',\n",
    " 'Family',\n",
    " 'Health (Life Expectancy)',\n",
    " 'Freedom',\n",
    " 'Trust (Government Corruption)',\n",
    " 'Generosity',\n",
    " 'Dystopia Residual',\n",
    " 'Country_Lab'\n",
    "], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the indexed dataframe to the dataframe output1\n",
    "output1 = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only required columns or features into final data dataframe\n",
    "final_data = output1.select(['features', 'Happiness Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the actual data into traindata & test data which is of 70% & 30%\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the LinearRegression module from the regression subpackage\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the label coulmn for this problem as 'Happiness Rank' and applying on LinearRegression function \n",
    "#and assigining to HR dataframe.\n",
    "HR = LinearRegression(labelCol='Happiness Rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fit the model on train data\n",
    "trained_HR_model = HR.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fit the model on test data and storing it to HR_results dataframe for measuring the accuracy of that model\n",
    "HR_result_1 = trained_HR_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the metric RMSE value\n",
    "HR_result_1.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the accuracy\n",
    "HR_result_1.r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression for 2017 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkSession is the entry point to Spark SQL. It is the very first object \n",
    "#to create while developing Spark SQL applications.\n",
    "#Used the SparkSession.builder method to create an instance of SparkSession with appName('WorldHealth2')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('WorldHealth2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the HR_comma_sep.csv file into df1 dataframe.\n",
    "df3 = spark.read.csv ('WH_2017.csv', inferSchema=True, header =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It displays the schema of the dataframe df1\n",
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the regularExpression module or function\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the column names in df3 with any dots('.') with '_' and placing it new dataframe 'df3_test'\n",
    "#I'm doing it as in pyspark it will not take or consider any dots in column name specification, so replacing with '_'.\n",
    "df3_test = df3.toDF(*(re.sub(r'[\\.\\s]+', '_', c) for c in df3.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the module StringIndexer from subpackage ml.feature\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In first step by using StringIndexer function we are creating an output label 'Country_Lab' with input 'Country', as we need integer values for performing logistic regression\n",
    "#We are transforming the above result into indexed dataframe and showing the first 10 results.\n",
    "indexer = StringIndexer(inputCol='Country', outputCol='Country_Lab')\n",
    "indexed = indexer.fit(df3_test).transform(df3_test)\n",
    "indexed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the modules Vectors & VectorAssembler from subpackage ml.linalg & ml.feature\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the columns in the indexed dataframe\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By using VectorAsssembler creating features output column with input columns including everything except label column\n",
    "#Happiness Rank and taken columns that re common with 2015 data.\n",
    "assembler = VectorAssembler(inputCols=[\n",
    " 'Happiness_Score',\n",
    " 'Economy_GDP_per_Capita_',\n",
    " 'Family',\n",
    " 'Health_Life_Expectancy_',\n",
    " 'Freedom',\n",
    " 'Generosity',\n",
    " 'Trust_Government_Corruption_',\n",
    " 'Dystopia_Residual',\n",
    " 'Country_Lab'\n",
    "], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the indexed dataframe to the dataframe output2\n",
    "output2 = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only required columns or features into final dataframe\n",
    "final = output2.select(['features', 'Happiness_Rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the actual data into traindata & test data which is of 70% & 30%\n",
    "train_data, test_data = final.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the LinearRegression module from the regression subpackage\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the label coulmn for this problem as 'Happiness_Rank' and applying on LinearRegression function \n",
    "#and assigining to HR_model dataframe.\n",
    "HR_model = LinearRegression(labelCol='Happiness_Rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fit the model on train data\n",
    "trained_model = HR_model.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fit the model on test data and storing it to HR_result_2 dataframe for measuring the accuracy of that model\n",
    "HR_result_2 = trained_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the metric RMSE value\n",
    "HR_result_2.rootMeanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Provides the accuracy\n",
    "HR_result_2.r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
