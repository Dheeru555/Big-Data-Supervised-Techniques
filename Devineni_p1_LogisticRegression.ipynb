{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SparkSession is the entry point to Spark SQL. It is the very first object \n",
    "#to create while developing Spark SQL applications.\n",
    "#Used the SparkSession.builder method to create an instance of SparkSession with appName('employee')\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('employee').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data from the HR_comma_sep.csv file into df dataframe.\n",
    "df = spark.read.csv ('HR_comma_sep.csv', inferSchema=True, header =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It displays the schema of the dataframe df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the module StringIndexer from subpackage ml.feature\n",
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In first step by using StringIndexer function we are creating an output label 'sal_label' with input 'salary', as we need integer values for performing logistic regression\n",
    "#We are transforming the above result into indexed dataframe and showing the first 10 results.\n",
    "indexer = StringIndexer(inputCol='salary', outputCol='sal_label')\n",
    "indexed = indexer.fit(df).transform(df)\n",
    "indexed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the modules Vectors & VectorAssembler from subpackage ml.linalg & ml.feature\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the columns in the indexed dataframe\n",
    "indexed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By using VectorAsssembler creating features output column with input columns including everything except label column\n",
    "#left and sales column which is not much required for this prediction.\n",
    "assembler = VectorAssembler(inputCols=[ 'satisfaction_level',\n",
    " 'last_evaluation',\n",
    " 'number_project',\n",
    " 'average_montly_hours',\n",
    " 'time_spend_company',\n",
    " 'Work_accident',\n",
    " 'promotion_last_5years',\n",
    " 'sal_label'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transforming the indexed dataframe to the dataframe output\n",
    "output = assembler.transform(indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only required columns or features into final data dataframe\n",
    "final_data = output.select(['features', 'left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the actual data into traindata & test data which is of 70% & 30%\n",
    "train_data, test_data = final_data.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the LogisticRegression module from the classification subpackage\n",
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the label coulmn for this problem as 'left' and applying on LogisticRegression function \n",
    "#and assigining to Employ_left dataframe.\n",
    "Employ_left=LogisticRegression(labelCol='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to fit the regression model on training data\n",
    "fitted_left_model = Employ_left.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the summary module on the fitted modeldataframe \n",
    "train_summary = fitted_left_model.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Show the prediction of actual label and prediction label\n",
    "train_summary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing BinaryClassificationEvaluator for evaluating the model\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tTrying to fit the model on test data and storing it to pred_and_labels dataframe for measuring the accuracy of thet model\n",
    "pred_and_labels = fitted_left_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It shows the prediction,probability along with the actual value.\n",
    "pred_and_labels.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the Evaluator on labelcoulmn and storing it into left_eval dataframe.\n",
    "left_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction', labelCol='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluating the default metric 'AreaUnderROC\"\n",
    "auc = left_eval.evaluate(pred_and_labels.predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the accuracy of 64%(0.6471)\n",
    "auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
